<div align="center">

# Software Engineering Arena

</div>

[Software Engineering Arena](https://huggingface.co/SWE-Arena) is an open-source initiative to transparently evaluate and track AI coding agents across real-world software engineering tasks. We provide interactive platforms, tracking systems, and novel metrics to advance the field of AI-assisted software development. 

> **â€œThe easier it is to verify a solution, the faster an AI system can learn to master the task.â€ â€” Jason WeiÂ¹**

Â¹ https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law

**Our mission:** We believe any evaluable task can eventually be automated with high-quality AI systems. We accelerate this transformation in software engineering by developing benchmarks and leaderboards that rigorously evaluate AI capabilities.

**Welcome collaboration from research labs, independent contributors, and the broader SE community!**

## ğŸŸï¸ [SWE-Model-Arena](https://github.com/Software-Engineering-Arena/SWE-Model-Arena): Interactive Multi-Round Model Evaluation

[![SWE-Model-Arena](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Model--Arena-blue?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/Software-Engineering-Arena)

An interactive platform for evaluating foundation models through **pairwise comparisons** in multi-round conversational workflows. Unlike static benchmarks, SWE-Model-Arena enables:

- **Multi-round dialogues** reflecting real-world SE interactions
- **Repository-aware context** via RepoChat for authentic evaluations
- **Novel metrics** including model consistency score and conversation efficiency index
- **Transparent leaderboard** with open-sourced ranking algorithms

Perfect for researchers and engineers seeking nuanced, context-aware assessments of AI models on software engineering tasks.

## ğŸ“Š GitHub-Based Agent Tracking Suite

Evaluate AI coding agents through their actual GitHub activity with our comprehensive tracking systems:

### [SWE-Commit](https://github.com/Software-Engineering-Arena/SWE-Commit) 
[![SWE-Commit](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Commit-red?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-Commit)

Track and analyze AI coding agents via their **commits**â€”measuring code quality, consistency, and contribution patterns.

### [SWE-PR](https://github.com/Software-Engineering-Arena/SWE-PR) 
[![SWE-PR](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--PR-purple?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-PR)

Track and analyze AI coding agents via their **pull requests**â€”examining merge success rates, discussion quality, and iterative improvements.

### [SWE-Review](https://github.com/Software-Engineering-Arena/SWE-Review) 
[![SWE-Review](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Review-green?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-Review)

Track and analyze AI coding agents via their **code reviews**â€”assessing feedback quality, issue identification, and collaborative capabilities.

### [SWE-Issue](https://github.com/Software-Engineering-Arena/SWE-Issue) 
[![SWE-Issue](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Issue-yellow?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-Issue)

Track and analyze AI coding agents via their **issue trackers**â€”from bug reports to feature requests and documentation.

## ğŸ¯ Our Mission

Software engineering extends far beyond code generationâ€”it encompasses requirements engineering, collaborative design, code review, debugging, and project management. Current evaluation frameworks often focus narrowly on code completion or generation. 

**Software Engineering Arena** provides:

- âœ… **Holistic evaluation** across diverse SE activities
- âœ… **Multi-turn interactions** matching real-world workflows  
- âœ… **Transparent methodologies** for reproducible research
- âœ… **Open-source tools** for community-driven innovation
- âœ… **Rich datasets** to advance AI-assisted software development

## ğŸ¤ Get Involved

We're actively seeking collaborators! Whether you're a:
- ğŸ”¬ **Researcher** developing new evaluation metrics
- ğŸ› ï¸ **Engineer** building AI coding tools
- ğŸ“Š **Data scientist** analyzing model performance
- ğŸŒ **Open source contributor** improving our platforms

**Ways to contribute:**
- Submit PRs to enhance our evaluation platforms
- Propose new metrics or tracking methodologies  
- Share datasets or evaluation results
- Report issues and suggest improvements
- Join discussions in our repositories

## ğŸ“„ License

All projects under Software Engineering Arena are licensed under the **Apache 2.0 License**. Data collected and open-sourced follows the same license.
