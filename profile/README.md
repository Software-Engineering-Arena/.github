<div align="center">

# Software Engineering Arena

</div>

[Software Engineering Arena](https://huggingface.co/SWE-Arena) is an open-source initiative to transparently evaluate and track AI coding agents across real-world software engineering tasks. We provide interactive platforms, tracking systems, and novel metrics to advance the field of AI-assisted software development. 

> **â€œThe easier it is to verify a solution, the faster an AI system can learn to master the task.â€ â€” Jason WeiÂ¹**

Â¹ https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law

**Our mission:** We believe any evaluable task can eventually be automated with high-quality AI systems. We accelerate this transformation in software engineering by developing benchmarks and leaderboards that rigorously evaluate AI capabilities.

**Welcome collaboration from research labs, independent contributors, and the broader SE community!**

## ğŸŸï¸ [SWE-Model-Arena](https://github.com/Software-Engineering-Arena/SWE-Model-Arena): Interactive Multi-Round Model Evaluation

[![SWE-Model-Arena](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Model--Arena-blue?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/Software-Engineering-Arena)

An interactive platform for evaluating foundation models through **pairwise comparisons** in multi-round conversational workflows. Unlike static benchmarks, SWE-Model-Arena enables:

- **Multi-round dialogues** reflecting real-world SE interactions
- **Repository-aware context** via RepoChat for authentic evaluations
- **Novel metrics** including model consistency score and conversation efficiency index
- **Transparent leaderboard** with open-sourced ranking algorithms

Perfect for researchers and engineers seeking nuanced, context-aware assessments of AI models on software engineering tasks.

## ğŸ“Š GitHub-Based Agent Tracking Suite

Evaluate AI coding agents through their actual GitHub activity with our comprehensive tracking systems:

### [SWE-PR](https://github.com/Software-Engineering-Arena/SWE-PR) 
[![SWE-PR](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--PR-purple?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-PR)

Track and analyze AI coding agents via their **pull requests**â€”examining merge rates, feature quality, and iterative improvements.

### [SWE-Review](https://github.com/Software-Engineering-Arena/SWE-Review) 
[![SWE-Review](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Review-green?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-Review)

Track and analyze AI coding agents via their **code reviews**â€”assessing issue identification, feedback timeliness, and collaborative atmosphere.

### [SWE-Issue](https://github.com/Software-Engineering-Arena/SWE-Issue) 
[![SWE-Issue](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Issue-yellow?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-Issue)

Track and analyze AI coding agents via their **issue trackers**â€”from bug reports to feature requests and product releases.

### [SWE-Discussion](https://github.com/Software-Engineering-Arena/SWE-Discussion) 
[![SWE-Discussion](https://img.shields.io/badge/ğŸŸï¸-Try%20SWE--Discussion-red?style=for-the-badge)](https://huggingface.co/spaces/SWE-Arena/SWE-Discussion)

Track and analyze AI coding agents via their **community discussions**â€”measuring question answering to polls and documentation.

## ğŸ“„ License

All projects under Software Engineering Arena are licensed under the **Apache 2.0 License**. Data collected and open-sourced follows the same license.
